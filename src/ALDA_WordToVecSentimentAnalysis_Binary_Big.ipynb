{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALDA_WordToVecSentimentAnalysis_Binary_Big",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2lHbkd-JIEC",
        "outputId": "b1e0946f-424b-40c5-989c-da42386caee7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-EgS8VOVL0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c59e5b-df52-4804-f0fb-97903e421ba0"
      },
      "source": [
        "import pandas as pd\n",
        "from gensim.models import word2vec\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize \n",
        "%pip install contractions\n",
        "import contractions\n",
        "import string\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from collections import Counter\n",
        "%pip install imbalanced-learn\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.0.58-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 15.3 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 54.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85437 sha256=9e922ba20afd0ce46f4fa5507a1cf880046986534ce1acde3018f89b33575f42\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.58 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY7CA-QTVja2"
      },
      "source": [
        "Importing a combined version of the dataset with 10k rows from each .tsv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcCW8QPdVNhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b2aaf2-970e-4f2b-adde-41fabfde224e"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/ALDA_Project/amazon_reviews_us_Electronics_v1_00.tsv',sep='\\t',error_bad_lines=False )"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "b'Skipping line 9076: expected 15 fields, saw 22\\nSkipping line 19256: expected 15 fields, saw 22\\nSkipping line 24313: expected 15 fields, saw 22\\nSkipping line 47211: expected 15 fields, saw 22\\nSkipping line 54295: expected 15 fields, saw 22\\nSkipping line 56641: expected 15 fields, saw 22\\nSkipping line 63067: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 93796: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 132806: expected 15 fields, saw 22\\nSkipping line 164631: expected 15 fields, saw 22\\nSkipping line 167019: expected 15 fields, saw 22\\nSkipping line 167212: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 198103: expected 15 fields, saw 22\\nSkipping line 199191: expected 15 fields, saw 22\\nSkipping line 202841: expected 15 fields, saw 22\\nSkipping line 218228: expected 15 fields, saw 22\\nSkipping line 235900: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 277761: expected 15 fields, saw 22\\nSkipping line 304582: expected 15 fields, saw 22\\nSkipping line 312029: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 343692: expected 15 fields, saw 22\\nSkipping line 352291: expected 15 fields, saw 22\\nSkipping line 363414: expected 15 fields, saw 22\\nSkipping line 378087: expected 15 fields, saw 22\\nSkipping line 378720: expected 15 fields, saw 22\\nSkipping line 378760: expected 15 fields, saw 22\\nSkipping line 379336: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 402682: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 466560: expected 15 fields, saw 22\\nSkipping line 486823: expected 15 fields, saw 22\\nSkipping line 489036: expected 15 fields, saw 22\\nSkipping line 496148: expected 15 fields, saw 22\\nSkipping line 522330: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 552961: expected 15 fields, saw 22\\nSkipping line 577388: expected 15 fields, saw 22\\nSkipping line 582182: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 590653: expected 15 fields, saw 22\\nSkipping line 608846: expected 15 fields, saw 22\\nSkipping line 615442: expected 15 fields, saw 22\\nSkipping line 645607: expected 15 fields, saw 22\\nSkipping line 654323: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 714935: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 749608: expected 15 fields, saw 22\\nSkipping line 753868: expected 15 fields, saw 22\\nSkipping line 762504: expected 15 fields, saw 22\\nSkipping line 771706: expected 15 fields, saw 22\\nSkipping line 773376: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 792407: expected 15 fields, saw 22\\nSkipping line 793933: expected 15 fields, saw 22\\nSkipping line 813269: expected 15 fields, saw 22\\nSkipping line 835491: expected 15 fields, saw 22\\nSkipping line 841176: expected 15 fields, saw 22\\nSkipping line 844604: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 857952: expected 15 fields, saw 22\\nSkipping line 859568: expected 15 fields, saw 22\\nSkipping line 860789: expected 15 fields, saw 22\\nSkipping line 863093: expected 15 fields, saw 22\\nSkipping line 881608: expected 15 fields, saw 22\\nSkipping line 891157: expected 15 fields, saw 22\\nSkipping line 893799: expected 15 fields, saw 22\\nSkipping line 906438: expected 15 fields, saw 22\\nSkipping line 914856: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 940736: expected 15 fields, saw 22\\nSkipping line 965818: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 993840: expected 15 fields, saw 22\\nSkipping line 1019036: expected 15 fields, saw 22\\nSkipping line 1019205: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 1058122: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 1144887: expected 15 fields, saw 22\\nSkipping line 1147255: expected 15 fields, saw 22\\nSkipping line 1164497: expected 15 fields, saw 22\\nSkipping line 1166930: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 1218319: expected 15 fields, saw 22\\nSkipping line 1232868: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 1307335: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 1621422: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 1857720: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 1935753: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 1988449: expected 15 fields, saw 22\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H2Pcr9-du7u"
      },
      "source": [
        "Removing all reviews with a neutral rating\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apG1C8EZdsXv",
        "outputId": "f635a76b-34e6-4098-e1ce-2a447f50e7d8"
      },
      "source": [
        "df = df[df.star_rating != 3]\n",
        "df.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2852637, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKyfgYbbVN-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3456387-05c6-48a5-b716-cb37eaf0cfad"
      },
      "source": [
        "df.loc[df['star_rating'] > 3, 'star_rating_sentiment'] = 'positive'\n",
        "\n",
        "df['star_rating_sentiment'] = df['star_rating_sentiment'].fillna('negative')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.obj[key] = _infer_fill_value(value)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN15rE7hVfs7"
      },
      "source": [
        "reviews = df[\"review_body\"].astype('str').values\n",
        "labels = df[\"star_rating_sentiment\"].values"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVTH0RE9NMRg"
      },
      "source": [
        "rus = RandomUnderSampler()\n",
        "reviews = reviews.reshape(-1, 1)\n",
        "reviews, labels = rus.fit_resample(reviews, labels)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6m0jA2wpdIr",
        "outputId": "a6644cab-684c-4931-dbe5-ebc3544d2ca7"
      },
      "source": [
        "len(reviews)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1073698"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tPX_2w-ZSRI"
      },
      "source": [
        "all_stopwords = STOPWORDS.union(['it','at','me','br','if','to','do'])\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "\n",
        "    tag = word[0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def process_review(sentence):\n",
        "\n",
        "  # convert the sentence to lower case, fix contrations and the otkenize\n",
        "  sentence = word_tokenize(contractions.fix(sentence.lower()))\n",
        "\n",
        "  # Removing all short words, punctuations and digits\n",
        "  sentence = [word for word in sentence if len(word)>1 and all(punc not in word for punc in string.punctuation) and not any(char.isdigit() for char in word)]\n",
        "\n",
        "  # remove all stop words\n",
        "  sentence = [word for word in sentence if not word in all_stopwords]\n",
        "\n",
        "  # add a parts of speech tag to each word\n",
        "  sentence = nltk.pos_tag(sentence)\n",
        "\n",
        "  # lemmatize each word using itself and its POS\n",
        "  sentence = [lemmatizer.lemmatize(word[0], get_wordnet_pos(word[1])) for word in sentence]\n",
        "\n",
        "  return sentence"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzbAs21zngc4"
      },
      "source": [
        "processed_reviews = []\n",
        "for review in reviews:\n",
        "  processed_reviews.append(process_review(review[0]))\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP5NUlRIpkc9"
      },
      "source": [
        "word_count = Counter()\n",
        "sentences_in_words = []\n",
        "for i in range(len(processed_reviews)):\n",
        "    sentences_in_words.append(processed_reviews[i])\n",
        "    \n",
        "    for word in sentences_in_words[i]:\n",
        "        \n",
        "        if len(word)>1 and type(word)==str and string.punctuation not in word:\n",
        "          word_count.update({str(word): 1})"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYeSiSGjCCum"
      },
      "source": [
        "model = word2vec.Word2Vec(sentences_in_words)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI0Qw4sECX1B",
        "outputId": "e9c4f093-a471-45ee-d943-ffe1e8994092"
      },
      "source": [
        "for word_pair in [['good','bad'],['best','worst'],['positive','negative'],['awesome','terrible']]:\n",
        "  result = []\n",
        "  correct_count = 0\n",
        "  for i in range(len(processed_reviews)):\n",
        "    sum_g = 0\n",
        "    sum_b = 0\n",
        "    n=0\n",
        "    for word in processed_reviews[i]:\n",
        "      try:\n",
        "        sum_g += model.wv.similarity(word, word_pair[0])\n",
        "        sum_b += model.wv.similarity(word, word_pair[1])\n",
        "      except:\n",
        "        n-=1\n",
        "    if sum_g>0:\n",
        "      avg_g = sum_g/(len(processed_reviews[i])+n)\n",
        "    else:\n",
        "      avg_g =0\n",
        "    if sum_b>0:\n",
        "      avg_b = sum_b/(len(processed_reviews[i])+n)\n",
        "    else:\n",
        "      avg_b =0\n",
        "    if (avg_g>avg_b and labels[i]=='positive') or (avg_g<=avg_b and labels[i]=='negative'):\n",
        "      correct_count+=1\n",
        "    # print(labels[i],avg_g,avg_b,avg_g>avg_b)\n",
        "  print(word_pair,correct_count/len(processed_reviews))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good', 'bad'] 0.7311376197031195\n",
            "['best', 'worst'] 0.6482791250426098\n",
            "['positive', 'negative'] 0.6003028784630315\n",
            "['awesome', 'terrible'] 0.7093149097791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjighiy7Ow1T",
        "outputId": "5f95b3d3-1b6f-4155-a68e-0d0c345a2d3d"
      },
      "source": [
        "for word_pair in [['good','bad'],['best','worst'],['positive','negative'],['awesome','terrible']]:\n",
        "  result = []\n",
        "  correct_count = 0\n",
        "  for i in range(len(processed_reviews)):\n",
        "    sum=0\n",
        "    for word in processed_reviews[i]:\n",
        "      try:\n",
        "        temp = (1 if model.wv.similarity(word, word_pair[0])>model.wv.similarity(word, word_pair[1]) else -1)\n",
        "        sum+= temp\n",
        "      except:\n",
        "        pass\n",
        "      \n",
        "    if (sum>0 and labels[i]=='positive') or (sum<=0 and labels[i]=='negative'):\n",
        "      correct_count+=1\n",
        "    # print(labels[i],avg_g,avg_b,avg_g>avg_b)\n",
        "  print(word_pair,correct_count/len(processed_reviews))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good', 'bad'] 0.6896725149902486\n",
            "['best', 'worst'] 0.6174818244981364\n",
            "['positive', 'negative'] 0.5987614766908386\n",
            "['awesome', 'terrible'] 0.6637406421544978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVTXyCZjT4fU"
      },
      "source": [
        "*   Good/bad = 0.694\n",
        "*   best/worst = 54.99%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8ZwgUSdPlv_",
        "outputId": "e7585516-1134-414e-8f8a-6feb96f333c0"
      },
      "source": [
        "for j in range(50,150,5):\n",
        "  result = []\n",
        "  correct_count = 0\n",
        "  for i in range(len(processed_reviews)):\n",
        "    sum_g = 0\n",
        "    sum_b = 0\n",
        "    n=0\n",
        "    for word in processed_reviews[i]:\n",
        "      try:\n",
        "        sum_g += model.wv.similarity(word, 'good')\n",
        "        sum_b += model.wv.similarity(word, 'bad')\n",
        "      except:\n",
        "        n-=1\n",
        "    if sum_g>0:\n",
        "      avg_g = sum_g/(len(processed_reviews[i])+n)\n",
        "    else:\n",
        "      avg_g =0\n",
        "    if sum_b>0:\n",
        "      avg_b = sum_b/(len(processed_reviews[i])+n)\n",
        "    else:\n",
        "      avg_b = 0\n",
        "    if (avg_b==0 and labels[i]=='positive'):\n",
        "      correct_count+=1\n",
        "    elif avg_b!=0:\n",
        "      if (avg_g/avg_b>j/100 and labels[i]=='positive') or (avg_g/avg_b<=j/100 and labels[i]=='negative'):\n",
        "        correct_count+=1\n",
        "\n",
        "  print('Threshold: ',j/100 ,' - Accuracy:',correct_count/len(processed_reviews))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold:  0.5  - Accuracy: 0.6471652177800462\n",
            "Threshold:  0.55  - Accuracy: 0.6564583337214003\n",
            "Threshold:  0.6  - Accuracy: 0.6657859100044892\n",
            "Threshold:  0.65  - Accuracy: 0.6750222129500102\n",
            "Threshold:  0.7  - Accuracy: 0.6841569975915015\n",
            "Threshold:  0.75  - Accuracy: 0.6931055101155074\n",
            "Threshold:  0.8  - Accuracy: 0.7018342215408803\n",
            "Threshold:  0.85  - Accuracy: 0.7098793142950811\n",
            "Threshold:  0.9  - Accuracy: 0.7173143658645168\n",
            "Threshold:  0.95  - Accuracy: 0.7242530022408535\n",
            "Threshold:  1.0  - Accuracy: 0.7302947383714974\n",
            "Threshold:  1.05  - Accuracy: 0.7356463363068573\n",
            "Threshold:  1.1  - Accuracy: 0.7400218683465928\n",
            "Threshold:  1.15  - Accuracy: 0.7431158482180278\n",
            "Threshold:  1.2  - Accuracy: 0.744866806122392\n",
            "Threshold:  1.25  - Accuracy: 0.7458903714079751\n",
            "Threshold:  1.3  - Accuracy: 0.7444299980068884\n",
            "Threshold:  1.35  - Accuracy: 0.7428904589558702\n",
            "Threshold:  1.4  - Accuracy: 0.740464264625621\n",
            "Threshold:  1.45  - Accuracy: 0.7365441679131376\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALDA_WordToVecSentimentAnalysis_Binary",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2lHbkd-JIEC",
        "outputId": "d4819473-b2d2-42ef-fcbf-19dcb68e9c1d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-EgS8VOVL0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c0f5c31-3656-4ccd-e4af-d11bb2869d12"
      },
      "source": [
        "import pandas as pd\n",
        "from gensim.models import word2vec\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize \n",
        "%pip install contractions\n",
        "import contractions\n",
        "import string\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from collections import Counter\n",
        "%pip install imbalanced-learn\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.0.58-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 40.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85439 sha256=e8bf84681b675f1ecdb53cfa41be3d5c68565cc532bd6f6383bd0024993f9a27\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.58 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY7CA-QTVja2"
      },
      "source": [
        "Importing a combined version of the dataset with 10k rows from each .tsv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcCW8QPdVNhK"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/ALDA_Project/amazon_reviews_us_combined.tsv',sep='\\t',error_bad_lines=False )"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H2Pcr9-du7u"
      },
      "source": [
        "Removing all reviews with a neutral rating\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apG1C8EZdsXv",
        "outputId": "f872f100-6048-4b90-a7f6-93ddbd203352"
      },
      "source": [
        "df = df[df.star_rating != 3]\n",
        "df.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(334435, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKyfgYbbVN-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420bdc86-a830-4892-e4ec-c0dbf812f1f7"
      },
      "source": [
        "df.loc[df['star_rating'] > 3, 'star_rating_sentiment'] = 'positive'\n",
        "\n",
        "df['star_rating_sentiment'] = df['star_rating_sentiment'].fillna('negative')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.obj[key] = _infer_fill_value(value)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1763: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN15rE7hVfs7"
      },
      "source": [
        "reviews = df[\"review_body\"].astype('str').values\n",
        "labels = df[\"star_rating_sentiment\"].values"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVTH0RE9NMRg"
      },
      "source": [
        "rus = RandomUnderSampler()\n",
        "reviews = reviews.reshape(-1, 1)\n",
        "reviews, labels = rus.fit_resample(reviews, labels)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0RgfyBApYJc",
        "outputId": "ebb8bb04-d2e5-4992-f163-8cd147ff44c1"
      },
      "source": [
        "len(reviews)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99332"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tPX_2w-ZSRI"
      },
      "source": [
        "all_stopwords = STOPWORDS.union(['it','at','me','br','if','to','do'])\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "\n",
        "    tag = word[0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def process_review(sentence):\n",
        "\n",
        "  # convert the sentence to lower case, fix contrations and the otkenize\n",
        "  sentence = word_tokenize(contractions.fix(sentence.lower()))\n",
        "\n",
        "  # Removing all short words, punctuations and digits\n",
        "  sentence = [word for word in sentence if len(word)>1 and all(punc not in word for punc in string.punctuation) and not any(char.isdigit() for char in word)]\n",
        "\n",
        "  # remove all stop words\n",
        "  sentence = [word for word in sentence if not word in all_stopwords]\n",
        "\n",
        "  # add a parts of speech tag to each word\n",
        "  sentence = nltk.pos_tag(sentence)\n",
        "\n",
        "  # lemmatize each word using itself and its POS\n",
        "  sentence = [lemmatizer.lemmatize(word[0], get_wordnet_pos(word[1])) for word in sentence]\n",
        "\n",
        "  return sentence"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzbAs21zngc4"
      },
      "source": [
        "processed_reviews = []\n",
        "for review in reviews:\n",
        "  processed_reviews.append(process_review(review[0]))\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP5NUlRIpkc9"
      },
      "source": [
        "word_count = Counter()\n",
        "sentences_in_words = []\n",
        "for i in range(len(processed_reviews)):\n",
        "    sentences_in_words.append(processed_reviews[i])\n",
        "    \n",
        "    for word in sentences_in_words[i]:\n",
        "        \n",
        "        if len(word)>1 and type(word)==str and string.punctuation not in word:\n",
        "          word_count.update({str(word): 1})"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYeSiSGjCCum"
      },
      "source": [
        "model = word2vec.Word2Vec(sentences_in_words)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI0Qw4sECX1B",
        "outputId": "5787aef6-bc0c-48c2-a3b1-478a0331edf5"
      },
      "source": [
        "for word_pair in [['good','bad'],['best','worst'],['positive','negative'],['awesome','terrible']]:\n",
        "  result = []\n",
        "  correct_count = 0\n",
        "  for i in range(len(processed_reviews)):\n",
        "    sum_g = 0\n",
        "    sum_b = 0\n",
        "    n=0\n",
        "    for word in processed_reviews[i]:\n",
        "      try:\n",
        "        sum_g += model.wv.similarity(word, word_pair[0])\n",
        "        sum_b += model.wv.similarity(word, word_pair[1])\n",
        "      except:\n",
        "        n-=1\n",
        "    if sum_g>0:\n",
        "      avg_g = sum_g/(len(processed_reviews[i])+n)\n",
        "    else:\n",
        "      avg_g =0\n",
        "    if sum_b>0:\n",
        "      avg_b = sum_b/(len(processed_reviews[i])+n)\n",
        "    else:\n",
        "      avg_b =0\n",
        "    if (avg_g>avg_b and labels[i]=='positive') or (avg_g<=avg_b and labels[i]=='negative'):\n",
        "      correct_count+=1\n",
        "    # print(labels[i],avg_g,avg_b,avg_g>avg_b)\n",
        "  print(word_pair,correct_count/len(processed_reviews))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good', 'bad'] 0.6631397736882374\n",
            "['best', 'worst'] 0.5676720492892522\n",
            "['positive', 'negative'] 0.5419099585229332\n",
            "['awesome', 'terrible'] 0.6695828131921234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjighiy7Ow1T",
        "outputId": "dade582e-03bc-444f-b541-ce40b4f3fd9e"
      },
      "source": [
        "for word_pair in [['good','bad'],['best','worst'],['positive','negative'],['awesome','terrible']]:\n",
        "  result = []\n",
        "  correct_count = 0\n",
        "  for i in range(len(processed_reviews)):\n",
        "    sum=0\n",
        "    for word in processed_reviews[i]:\n",
        "      try:\n",
        "        temp = (1 if model.wv.similarity(word, word_pair[0])>model.wv.similarity(word, word_pair[1]) else -1)\n",
        "        sum+= temp\n",
        "      except:\n",
        "        pass\n",
        "      \n",
        "    if (sum>0 and labels[i]=='positive') or (sum<=0 and labels[i]=='negative'):\n",
        "      correct_count+=1\n",
        "    # print(labels[i],avg_g,avg_b,avg_g>avg_b)\n",
        "  print(word_pair,correct_count/len(processed_reviews))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good', 'bad'] 0.6237466274715097\n",
            "['best', 'worst'] 0.5518664680062819\n",
            "['positive', 'negative'] 0.5527221841903918\n",
            "['awesome', 'terrible'] 0.6340756251761769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8ZwgUSdPlv_",
        "outputId": "2585873f-b543-425a-ddbc-8166203ba54c"
      },
      "source": [
        "for j in range(50,150,5):\n",
        "  result = []\n",
        "  correct_count = 0\n",
        "  for i in range(len(processed_reviews)):\n",
        "    sum_g = 0\n",
        "    sum_b = 0\n",
        "    n=0\n",
        "    for word in processed_reviews[i]:\n",
        "      try:\n",
        "        sum_g += model.wv.similarity(word, 'good')\n",
        "        sum_b += model.wv.similarity(word, 'bad')\n",
        "      except:\n",
        "        n-=1\n",
        "    if sum_g>0:\n",
        "      avg_g = sum_g/(len(processed_reviews[i])+n)\n",
        "    else:\n",
        "      avg_g =0\n",
        "    if sum_b>0:\n",
        "      avg_b = sum_b/(len(processed_reviews[i])+n)\n",
        "    else:\n",
        "      avg_b = 0\n",
        "    if (avg_b==0 and labels[i]=='positive'):\n",
        "      correct_count+=1\n",
        "    elif avg_b!=0:\n",
        "      if (avg_g/avg_b>j/100 and labels[i]=='positive') or (avg_g/avg_b<=j/100 and labels[i]=='negative'):\n",
        "        correct_count+=1\n",
        "\n",
        "  print('Threshold: ',j/100 ,' - Accuracy:',correct_count/len(processed_reviews))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold:  0.5  - Accuracy: 0.523335883703137\n",
            "Threshold:  0.55  - Accuracy: 0.531017194861676\n",
            "Threshold:  0.6  - Accuracy: 0.5400273829178915\n",
            "Threshold:  0.65  - Accuracy: 0.5514235090403898\n",
            "Threshold:  0.7  - Accuracy: 0.5661720291547537\n",
            "Threshold:  0.75  - Accuracy: 0.5832561510892764\n",
            "Threshold:  0.8  - Accuracy: 0.6010852494664358\n",
            "Threshold:  0.85  - Accuracy: 0.6196190552893327\n",
            "Threshold:  0.9  - Accuracy: 0.6383944750936255\n",
            "Threshold:  0.95  - Accuracy: 0.6552973865421012\n",
            "Threshold:  1.0  - Accuracy: 0.6698445616719687\n",
            "Threshold:  1.05  - Accuracy: 0.682146740224701\n",
            "Threshold:  1.1  - Accuracy: 0.6911468610316917\n",
            "Threshold:  1.15  - Accuracy: 0.6987375669472073\n",
            "Threshold:  1.2  - Accuracy: 0.7019490194499255\n",
            "Threshold:  1.25  - Accuracy: 0.7041336125317118\n",
            "Threshold:  1.3  - Accuracy: 0.7026235251479885\n",
            "Threshold:  1.35  - Accuracy: 0.6992912656545726\n",
            "Threshold:  1.4  - Accuracy: 0.6932710506181291\n",
            "Threshold:  1.45  - Accuracy: 0.687401844320058\n"
          ]
        }
      ]
    }
  ]
}